{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "honey-trick",
   "metadata": {},
   "source": [
    "# Machine learning is making great progress in understanding our world \n",
    "*Where we use the magic of word embeddings to illustrate increased semantic understanding in machine learning models*\n",
    "\n",
    "*The article, code and data can be [found on github](https://github.com/PaulHiemstra/wordembedding_magic)*\n",
    "\n",
    "# Introduction\n",
    "- Humans have an intuitive internal model of how the world works: if we let go of something it drops, if we pick up an egg we know not to squeeze too hard. We have *semantic* understanding, we know what things mean. \n",
    "- A consequence of this is that we do not need to experience every situation directly to make smart decisions. We all learn that eggs are fragile, without every having to have thrown an egg at the wall. \n",
    "- Being able to reason from our internal model into the world allows the average human to learn how to drive in 50 hours versus millions of hours of training time for an self driving AI. \n",
    "- Machine learning models are nowhere near this kind of internal model and require copious amounts of examples to learn from. However, strides have been made towards ML models having more semantic understanding. \n",
    "- A great example are word embeddings. These shows a representation of text data that hints towards a limited underlying 'internal model'.\n",
    "- In this article I will pick apart a set of trained word embeddings and make a case that this shows increased semantic understanding. \n",
    "\n",
    "# What are word embeddings\n",
    "- This will be a very short description, see source x and source y for the real details. \n",
    "\n",
    "# Loading pre-trained Glove Word embeddings\n",
    "\n",
    "# Show that the word embeddings show a degree of semantic understanding\n",
    "\n",
    "# Conclusion\n",
    "- Reference things like transformers who take this kind of semantic understanding to the next level.\n",
    "- Is it really semantic understanding? Or some kind of sufficiently complex layered abstractions that seem to us to be 'understanding'. The issue here is that we don't really understand how our own mind and internal representation work, and wether or not there is something special and unique below complex layers of rules and abstractions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
